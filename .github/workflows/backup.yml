name: Daily Supabase Backup to R2

on:
  schedule:
    # Run at 2:00 AM IST (20:30 UTC previous day)
    - cron: '30 20 * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub Actions UI

env:
  R2_BUCKET: supabase-backups
  RETENTION_DAYS: 30

jobs:
  backup:
    name: Backup Supabase DB
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install PostgreSQL 17 client
        run: |
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17

      - name: Configure AWS CLI for R2
        run: |
          aws configure set aws_access_key_id ${{ secrets.R2_ACCESS_KEY_ID }} --profile r2
          aws configure set aws_secret_access_key ${{ secrets.R2_SECRET_ACCESS_KEY }} --profile r2
          aws configure set region auto --profile r2

      - name: Create database backup
        env:
          DATABASE_URL: ${{ secrets.SUPABASE_BACKUP_URL }}
        run: |
          TIMESTAMP=$(date +"%Y-%m-%d_%H-%M-%S")
          BACKUP_FILE="backup_${TIMESTAMP}.dump"
          
          echo "üì¶ Creating backup: ${BACKUP_FILE}"
          pg_dump "${DATABASE_URL}" \
            --format=custom \
            --no-owner \
            --no-acl \
            --file="/tmp/${BACKUP_FILE}"
          
          FILESIZE=$(du -h "/tmp/${BACKUP_FILE}" | cut -f1)
          echo "‚úÖ Backup created: ${FILESIZE}"
          
          echo "BACKUP_FILE=${BACKUP_FILE}" >> $GITHUB_ENV
          echo "BACKUP_PATH=/tmp/${BACKUP_FILE}" >> $GITHUB_ENV
          echo "FILESIZE=${FILESIZE}" >> $GITHUB_ENV

      - name: Upload to Cloudflare R2
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          echo "‚òÅÔ∏è Uploading to R2..."
          aws s3 cp "${{ env.BACKUP_PATH }}" \
            "s3://${{ env.R2_BUCKET }}/daily/${{ env.BACKUP_FILE }}" \
            --endpoint-url "${{ secrets.R2_ENDPOINT }}" \
            --profile r2
          
          echo "‚úÖ Uploaded: s3://${{ env.R2_BUCKET }}/daily/${{ env.BACKUP_FILE }}"

      - name: Cleanup old backups
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          CUTOFF_DATE=$(date -d "-${{ env.RETENTION_DAYS }} days" +%Y-%m-%d)
          echo "üßπ Removing backups older than ${CUTOFF_DATE}..."
          
          aws s3 ls "s3://${{ env.R2_BUCKET }}/daily/" \
            --endpoint-url "${{ secrets.R2_ENDPOINT }}" \
            --profile r2 | while read -r line; do
            
            FILE_DATE=$(echo "$line" | awk '{print $1}')
            FILE_NAME=$(echo "$line" | awk '{print $4}')
            
            if [[ -n "${FILE_NAME}" && "${FILE_DATE}" < "${CUTOFF_DATE}" ]]; then
              echo "  üóëÔ∏è Deleting: ${FILE_NAME}"
              aws s3 rm "s3://${{ env.R2_BUCKET }}/daily/${FILE_NAME}" \
                --endpoint-url "${{ secrets.R2_ENDPOINT }}" \
                --profile r2
            fi
          done
          
          echo "‚úÖ Cleanup complete"

      - name: Backup summary
        run: |
          echo ""
          echo "============================================"
          echo "üéâ BACKUP COMPLETE!"
          echo "============================================"
          echo "üìÅ File: ${{ env.BACKUP_FILE }}"
          echo "üìä Size: ${{ env.FILESIZE }}"
          echo "üìç Bucket: ${{ env.R2_BUCKET }}"
          echo "üïê Retention: ${{ env.RETENTION_DAYS }} days"
          echo "============================================"
